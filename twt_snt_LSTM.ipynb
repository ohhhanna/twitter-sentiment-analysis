{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1206s3WaeIGr4j8EeiANCahfRO4tFwejq",
      "authorship_tag": "ABX9TyP0iJa8UvV5GwNlL8tGDlAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohhhanna/twitter-sentiment-analysis/blob/main/twt_snt_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwxmdFgwiW5Q",
        "outputId": "2ff6923e-db09-4ded-c0ae-f9fd84aa8b8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GZyR6POxiYNa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_file = open('/content/drive/My Drive/X_train.pkl', 'rb')\n",
        "X_train = pickle.load(x_train_file)\n",
        "\n",
        "x_test_file = open('/content/drive/My Drive/X_test.pkl', 'rb')\n",
        "X_test = pickle.load(x_test_file)\n",
        "\n",
        "y_train_file = open('/content/drive/My Drive/Y_train.pkl', 'rb')\n",
        "Y_train = pickle.load(y_train_file)\n",
        "\n",
        "y_test_file = open('/content/drive/My Drive/Y_test.pkl', 'rb')\n",
        "Y_test = pickle.load(y_test_file)\n",
        "\n",
        "x_val_file = open('/content/drive/My Drive/X_val.pkl', 'rb')\n",
        "X_val = pickle.load(x_val_file)\n",
        "\n",
        "y_val_file = open('/content/drive/My Drive/Y_val.pkl', 'rb')\n",
        "Y_val = pickle.load(y_val_file)\n",
        "\n",
        "#rb= read binary"
      ],
      "metadata": {
        "id": "YYHFSkaKiabI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = []\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/glove.6B.50d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        #we are just spliting the words and aooending them, giving them an index and embedding them with float64 and outting them in a vector\n",
        "\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float64)\n",
        "\n",
        "        vectors.append(vect)\n",
        "#putting the vectors in glove\n",
        "glove = {w: vectors[word2idx[w]] for w in words}\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "GsEr-xLcifr-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save GloVe dictionary.\n",
        "glove_file = open(\"/content/drive/My Drive/glove_dict_50d.pkl\", \"wb\")\n",
        "pickle.dump(glove,glove_file)\n",
        "glove_file.close()\n",
        "\n",
        "# Load GloVe dictionary.\n",
        "glove_file = open(\"/content/drive/My Drive/glove_dict_50d.pkl\", \"rb\")\n",
        "glove = pickle.load(glove_file)\n",
        "# glove_file.close()"
      ],
      "metadata": {
        "id": "rGOrPvViii7M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Vectors_from_Glove(dataset):\n",
        "  matrix_len = dataset['text'].str.split().str.len().sum()\n",
        "  # print(matrix_len)\n",
        "  words_found = 0\n",
        "  words = 0\n",
        "  mean_tweets = []\n",
        "\n",
        "  for i,row in enumerate(dataset['text']):\n",
        "    splited_words = str(row).split(' ')\n",
        "    weight_matrix = np.zeros((len(splited_words),50))\n",
        "    for j,word in enumerate(splited_words):\n",
        "      words += 1\n",
        "      if word in glove:\n",
        "        weight_matrix[j] = glove[word]\n",
        "        words_found += 1\n",
        "    mean_tweets.append(np.mean(weight_matrix,axis=0))\n",
        "\n",
        "  return np.array(mean_tweets), words_found, matrix_len"
      ],
      "metadata": {
        "id": "Xl3JzPIiipXU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gloveX_train, words_found, matrix_len = Vectors_from_Glove(pd.DataFrame(X_train))\n",
        "print(gloveX_train.shape)\n",
        "# print(type(gloveX_train))\n",
        "# print(len(gloveX_train[0]))\n",
        "# print(gloveX_train[0])\n",
        "print(\"Ratio of words from x_train that have founded in Glove: \", round(words_found/matrix_len,2))\n",
        "\n",
        "gloveX_val, words_found, matrix_len = Vectors_from_Glove(pd.DataFrame(X_val))\n",
        "print(\"Ratio of words from x_val that have founded in Glove: \", round(words_found/matrix_len,2))\n",
        "\n",
        "gloveX_test, words_found, matrix_len = Vectors_from_Glove(pd.DataFrame(X_test))\n",
        "print(\"Ratio of words from x_test that have founded in Glove: \", round(words_found/matrix_len,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G109-Ogiq00",
        "outputId": "c5f6f733-a8b0-4e4b-8079-f9c264b9028f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(495012, 50)\n",
            "Ratio of words from x_train that have founded in Glove:  0.93\n",
            "Ratio of words from x_val that have founded in Glove:  0.92\n",
            "Ratio of words from x_test that have founded in Glove:  0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X datasets to tensors with function convert_to_tensor.\n",
        "gloveX_train = torch.tensor(gloveX_train)\n",
        "gloveX_val = torch.tensor(gloveX_val)\n",
        "gloveX_test = torch.tensor(gloveX_test)"
      ],
      "metadata": {
        "id": "IL8Yjn5ziuD9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Y labels to tensors with torch.squeeze.\n",
        "Y_train = torch.squeeze(torch.from_numpy(Y_train.to_numpy()).float())\n",
        "Y_val = torch.squeeze(torch.from_numpy(Y_val.to_numpy()).float())\n",
        "Y_test = torch.squeeze(torch.from_numpy(Y_test.to_numpy()).float())"
      ],
      "metadata": {
        "id": "zBW8TFsoi2DA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2dssMz-i4Le",
        "outputId": "27957ab2-38f7-4578-feab-779d04d8e609"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class LSTM_Model(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTM_Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x.view(len(x), 1, -1))\n",
        "        output = self.fc(lstm_out.view(len(x), -1))\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n",
        "\n",
        "# Assuming gloveX_train is your input data\n",
        "input_dim = gloveX_train.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = 1\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "lstm_model = LSTM_Model(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Move model to GPU if available\n",
        "lstm_model.to(device)\n",
        "criterion = criterion.to(device)\n"
      ],
      "metadata": {
        "id": "GbQGO5ZPSIll"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming you have your training data in gloveX_train and corresponding labels in Y_train\n",
        "train_data = TensorDataset(gloveX_train, Y_train)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define number of epochs\n",
        "num_epochs = 80\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    lstm_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Convert inputs to torch.float32\n",
        "        inputs = inputs.to(torch.float32)\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = lstm_model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))  # Ensure labels have correct shape\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_labels = (outputs > 0.5).float()\n",
        "        correct_predictions += (predicted_labels == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3g4ykMSTpZP",
        "outputId": "2ee89b0c-3196-46c8-a076-be2cb2f49854"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80, Loss: 0.6234, Accuracy: 0.6544\n",
            "Epoch 2/80, Loss: 0.6039, Accuracy: 0.6703\n",
            "Epoch 3/80, Loss: 0.6027, Accuracy: 0.6708\n",
            "Epoch 4/80, Loss: 0.6019, Accuracy: 0.6716\n",
            "Epoch 5/80, Loss: 0.6012, Accuracy: 0.6718\n",
            "Epoch 6/80, Loss: 0.6005, Accuracy: 0.6724\n",
            "Epoch 7/80, Loss: 0.5998, Accuracy: 0.6731\n",
            "Epoch 8/80, Loss: 0.5991, Accuracy: 0.6734\n",
            "Epoch 9/80, Loss: 0.5984, Accuracy: 0.6745\n",
            "Epoch 10/80, Loss: 0.5975, Accuracy: 0.6750\n",
            "Epoch 11/80, Loss: 0.5967, Accuracy: 0.6758\n",
            "Epoch 12/80, Loss: 0.5959, Accuracy: 0.6767\n",
            "Epoch 13/80, Loss: 0.5952, Accuracy: 0.6774\n",
            "Epoch 14/80, Loss: 0.5944, Accuracy: 0.6778\n",
            "Epoch 15/80, Loss: 0.5937, Accuracy: 0.6785\n",
            "Epoch 16/80, Loss: 0.5930, Accuracy: 0.6797\n",
            "Epoch 17/80, Loss: 0.5922, Accuracy: 0.6797\n",
            "Epoch 18/80, Loss: 0.5916, Accuracy: 0.6805\n",
            "Epoch 19/80, Loss: 0.5909, Accuracy: 0.6812\n",
            "Epoch 20/80, Loss: 0.5903, Accuracy: 0.6817\n",
            "Epoch 21/80, Loss: 0.5898, Accuracy: 0.6821\n",
            "Epoch 22/80, Loss: 0.5893, Accuracy: 0.6822\n",
            "Epoch 23/80, Loss: 0.5887, Accuracy: 0.6833\n",
            "Epoch 24/80, Loss: 0.5882, Accuracy: 0.6835\n",
            "Epoch 25/80, Loss: 0.5877, Accuracy: 0.6836\n",
            "Epoch 26/80, Loss: 0.5872, Accuracy: 0.6845\n",
            "Epoch 27/80, Loss: 0.5867, Accuracy: 0.6847\n",
            "Epoch 28/80, Loss: 0.5863, Accuracy: 0.6851\n",
            "Epoch 29/80, Loss: 0.5858, Accuracy: 0.6854\n",
            "Epoch 30/80, Loss: 0.5854, Accuracy: 0.6857\n",
            "Epoch 31/80, Loss: 0.5850, Accuracy: 0.6863\n",
            "Epoch 32/80, Loss: 0.5845, Accuracy: 0.6863\n",
            "Epoch 33/80, Loss: 0.5841, Accuracy: 0.6867\n",
            "Epoch 34/80, Loss: 0.5837, Accuracy: 0.6871\n",
            "Epoch 35/80, Loss: 0.5834, Accuracy: 0.6872\n",
            "Epoch 36/80, Loss: 0.5829, Accuracy: 0.6877\n",
            "Epoch 37/80, Loss: 0.5825, Accuracy: 0.6883\n",
            "Epoch 38/80, Loss: 0.5821, Accuracy: 0.6884\n",
            "Epoch 39/80, Loss: 0.5818, Accuracy: 0.6888\n",
            "Epoch 40/80, Loss: 0.5814, Accuracy: 0.6887\n",
            "Epoch 41/80, Loss: 0.5811, Accuracy: 0.6891\n",
            "Epoch 42/80, Loss: 0.5807, Accuracy: 0.6899\n",
            "Epoch 43/80, Loss: 0.5803, Accuracy: 0.6901\n",
            "Epoch 44/80, Loss: 0.5801, Accuracy: 0.6901\n",
            "Epoch 45/80, Loss: 0.5796, Accuracy: 0.6904\n",
            "Epoch 46/80, Loss: 0.5793, Accuracy: 0.6905\n",
            "Epoch 47/80, Loss: 0.5791, Accuracy: 0.6910\n",
            "Epoch 48/80, Loss: 0.5787, Accuracy: 0.6910\n",
            "Epoch 49/80, Loss: 0.5784, Accuracy: 0.6917\n",
            "Epoch 50/80, Loss: 0.5782, Accuracy: 0.6915\n",
            "Epoch 51/80, Loss: 0.5779, Accuracy: 0.6917\n",
            "Epoch 52/80, Loss: 0.5776, Accuracy: 0.6919\n",
            "Epoch 53/80, Loss: 0.5774, Accuracy: 0.6919\n",
            "Epoch 54/80, Loss: 0.5770, Accuracy: 0.6928\n",
            "Epoch 55/80, Loss: 0.5768, Accuracy: 0.6925\n",
            "Epoch 56/80, Loss: 0.5766, Accuracy: 0.6928\n",
            "Epoch 57/80, Loss: 0.5762, Accuracy: 0.6930\n",
            "Epoch 58/80, Loss: 0.5760, Accuracy: 0.6931\n",
            "Epoch 59/80, Loss: 0.5759, Accuracy: 0.6931\n",
            "Epoch 60/80, Loss: 0.5755, Accuracy: 0.6935\n",
            "Epoch 61/80, Loss: 0.5753, Accuracy: 0.6937\n",
            "Epoch 62/80, Loss: 0.5752, Accuracy: 0.6938\n",
            "Epoch 63/80, Loss: 0.5750, Accuracy: 0.6937\n",
            "Epoch 64/80, Loss: 0.5748, Accuracy: 0.6941\n",
            "Epoch 65/80, Loss: 0.5746, Accuracy: 0.6941\n",
            "Epoch 66/80, Loss: 0.5744, Accuracy: 0.6941\n",
            "Epoch 67/80, Loss: 0.5742, Accuracy: 0.6947\n",
            "Epoch 68/80, Loss: 0.5740, Accuracy: 0.6947\n",
            "Epoch 69/80, Loss: 0.5739, Accuracy: 0.6948\n",
            "Epoch 70/80, Loss: 0.5737, Accuracy: 0.6950\n",
            "Epoch 71/80, Loss: 0.5736, Accuracy: 0.6949\n",
            "Epoch 72/80, Loss: 0.5732, Accuracy: 0.6953\n",
            "Epoch 73/80, Loss: 0.5731, Accuracy: 0.6951\n",
            "Epoch 74/80, Loss: 0.5730, Accuracy: 0.6954\n",
            "Epoch 75/80, Loss: 0.5729, Accuracy: 0.6956\n",
            "Epoch 76/80, Loss: 0.5727, Accuracy: 0.6957\n",
            "Epoch 77/80, Loss: 0.5726, Accuracy: 0.6960\n",
            "Epoch 78/80, Loss: 0.5724, Accuracy: 0.6962\n",
            "Epoch 79/80, Loss: 0.5722, Accuracy: 0.6965\n",
            "Epoch 80/80, Loss: 0.5721, Accuracy: 0.6962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming you have your test data in gloveX_test and corresponding labels in Y_test\n",
        "test_data = TensorDataset(gloveX_test, Y_test)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Create DataLoader\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Testing loop\n",
        "lstm_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(torch.float32)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = lstm_model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1))  # Ensure labels have correct shape\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_labels = (outputs > 0.5).float()\n",
        "        correct_predictions += (predicted_labels == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "test_loss = test_loss / total_samples\n",
        "test_accuracy = correct_predictions / total_samples\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yCmh0OtcKFv",
        "outputId": "90d835b4-a85f-4363-95aa-c57cc782712e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5676, Test Accuracy: 0.6960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LILQ7MyCcqT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}