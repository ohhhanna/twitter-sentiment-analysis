{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIJPAype47fzYhWjrAVJA+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohhhanna/twitter-sentiment-analysis/blob/main/sentiment_analysis_ff_GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJgnUbFqVm0-",
        "outputId": "fd141a90-14e1-4b4c-c4a3-e62c6bb784da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "BQ9180xPbtgV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS7ImoOyb7Mk",
        "outputId": "0f9cb057-70ad-41f6-a9b3-8a9c2c5a87da"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the prepocessed data"
      ],
      "metadata": {
        "id": "mo1fp3evcByG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_file = open('/content/drive/My Drive/X_train.pkl', 'rb')\n",
        "X_train = pickle.load(x_train_file)\n",
        "\n",
        "x_test_file = open('/content/drive/My Drive/X_test.pkl', 'rb')\n",
        "X_test = pickle.load(x_test_file)\n",
        "\n",
        "y_train_file = open('/content/drive/My Drive/Y_train.pkl', 'rb')\n",
        "Y_train = pickle.load(y_train_file)\n",
        "\n",
        "y_test_file = open('/content/drive/My Drive/Y_test.pkl', 'rb')\n",
        "Y_test = pickle.load(y_test_file)\n",
        "\n",
        "x_val_file = open('/content/drive/My Drive/X_val.pkl', 'rb')\n",
        "X_val = pickle.load(x_val_file)\n",
        "\n",
        "y_val_file = open('/content/drive/My Drive/Y_val.pkl', 'rb')\n",
        "Y_val = pickle.load(y_val_file)\n",
        "\n",
        "#rb= read binary"
      ],
      "metadata": {
        "id": "eJlUINjwcGUH"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VECTORIZATION WITH PRETRAINED EMBEDDING VECTORS USING GloVe"
      ],
      "metadata": {
        "id": "usrhliN7cROf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "use of glove.6B.50d.text file,\n",
        "has 400 words\n",
        "represented by 50 dimension vectors\n"
      ],
      "metadata": {
        "id": "-msxxYntcanN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = []\n",
        "\n",
        "with open(f'/content/drive/MyDrive/Colab Notebooks/glove.6B.50d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        #we are just spliting the words and aooending them, giving them an index and embedding them with float64 and outting them in a vector\n",
        "\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float64)\n",
        "\n",
        "        vectors.append(vect)\n",
        "#putting the vectors in glove\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "metadata": {
        "id": "k_QifcrkcQfB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save GloVe dictionary.\n",
        "glove_file = open(\"/content/drive/My Drive/glove_dict_50d.pkl\", \"wb\")\n",
        "pickle.dump(glove,glove_file)\n",
        "glove_file.close()\n",
        "\n",
        "# Load GloVe dictionary.\n",
        "glove_file = open(\"/content/drive/My Drive/glove_dict_50d.pkl\", \"rb\")\n",
        "glove = pickle.load(glove_file)\n",
        "# glove_file.close()"
      ],
      "metadata": {
        "id": "1Bnq6Svifmq2"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build a matrix of weights where shape will be equal to (dataset’s vocabulary length, word vectors dimension)."
      ],
      "metadata": {
        "id": "LmSatzN4gGpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each word in dataset’s vocabulary, check if it is on glove’s vocabulary. If there is, load its pre-trained word vector. Otherwise,initialize it with zeros. Finally,find the mean vector of each tweet"
      ],
      "metadata": {
        "id": "Ehgmp5_IgLtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Vectors_from_Glove(dataset):\n",
        "  matrix_len = dataset['text'].str.split().str.len().sum()\n",
        "  # print(matrix_len)\n",
        "  words_found = 0\n",
        "  words = 0\n",
        "  mean_tweets = []\n",
        "\n",
        "  for i,row in enumerate(dataset['text']):\n",
        "    splited_words = str(row).split(' ')\n",
        "    weight_matrix = np.zeros((len(splited_words),50))\n",
        "    for j,word in enumerate(splited_words):\n",
        "      words += 1\n",
        "      if word in glove:\n",
        "        weight_matrix[j] = glove[word]\n",
        "        words_found += 1\n",
        "    mean_tweets.append(np.mean(weight_matrix,axis=0))\n",
        "\n",
        "  return np.array(mean_tweets), words_found, matrix_len"
      ],
      "metadata": {
        "id": "huFkXTFPfsvT"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the ratio of words that founded in glove on train, validation and test set."
      ],
      "metadata": {
        "id": "ibQdrF67gWhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gloveX_train, words_found, matrix_len = Vectors_from_Glove(pd.DataFrame(X_train))\n",
        "print(gloveX_train.shape)\n",
        "# print(type(gloveX_train))\n",
        "# print(len(gloveX_train[0]))\n",
        "# print(gloveX_train[0])\n",
        "print(\"Ratio of words from x_train that have founded in Glove: \", round(words_found/matrix_len,2))\n",
        "\n",
        "gloveX_val, words_found, matrix_len = Vectors_from_Glove(pd.DataFrame(X_val))\n",
        "print(\"Ratio of words from x_val that have founded in Glove: \", round(words_found/matrix_len,2))\n",
        "\n",
        "gloveX_test, words_found, matrix_len = Vectors_from_Glove(pd.DataFrame(X_test))\n",
        "print(\"Ratio of words from x_test that have founded in Glove: \", round(words_found/matrix_len,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEMYdT1BgdGu",
        "outputId": "142c8dbc-98ea-4957-dc4a-1a770e0ed45f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(495012, 50)\n",
            "Ratio of words from x_train that have founded in Glove:  0.93\n",
            "Ratio of words from x_val that have founded in Glove:  0.92\n",
            "Ratio of words from x_test that have founded in Glove:  0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**convert to tensors**"
      ],
      "metadata": {
        "id": "Cw06fbrBglBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert X datasets to tensors with function convert_to_tensor.\n",
        "gloveX_train = torch.tensor(gloveX_train)\n",
        "gloveX_val = torch.tensor(gloveX_val)\n",
        "gloveX_test = torch.tensor(gloveX_test)"
      ],
      "metadata": {
        "id": "RpB9BT5jgowd"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Y labels to tensors with torch.squeeze.\n",
        "Y_train = torch.squeeze(torch.from_numpy(Y_train.to_numpy()).float())\n",
        "Y_val = torch.squeeze(torch.from_numpy(Y_val.to_numpy()).float())\n",
        "Y_test = torch.squeeze(torch.from_numpy(Y_test.to_numpy()).float())"
      ],
      "metadata": {
        "id": "YIGNQtIsgrrz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5YMvo-jgvaP",
        "outputId": "4207a045-048b-4152-da19-9e888aea7f6b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Feed_forward_netModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_1, hidden_2, output_dim):\n",
        "        super(Feed_forward_netModel, self).__init__()\n",
        "\n",
        "        # First linear function\n",
        "        self.layer_1 = nn.Linear(input_dim, hidden_1)\n",
        "        # Non-linearity\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        # Second linear function\n",
        "        self.layer_2 = nn.Linear(hidden_1, hidden_2)\n",
        "        # Non-linearity 2\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        # Output linear layer\n",
        "        self.layer_3 = nn.Linear(hidden_2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer_1(x)\n",
        "        out = self.relu_1(out)\n",
        "        out = self.layer_2(out)\n",
        "        out = self.relu_2(out)\n",
        "        out = self.layer_3(out)\n",
        "        return torch.sigmoid(out)\n"
      ],
      "metadata": {
        "id": "9lqohcrwg3w1"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensions of each layer and num of epochs.\n",
        "input_dim = gloveX_train.shape[1]\n",
        "hidden_dim_1 = 64\n",
        "hidden_dim_2 = 16\n",
        "output_dim = 1\n",
        "num_epochs = 80\n",
        "\n",
        "#define model\n",
        "ff_model = Feed_forward_netModel(input_dim,hidden_dim_1,hidden_dim_2,output_dim)\n",
        "\n",
        "# Define loss function.\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define as optimizer Adam.\n",
        "optimizer = optim.Adam(ff_model.parameters(),lr=1e-4,weight_decay=1e-4)\n",
        "\n",
        "# Transfer all the computation to GPU (cuda device).\n",
        "ff_model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "pmgRG4HIiGSB"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "  y_pred = torch.round(y_pred)\n",
        "  correct = (y_true == y_pred).float()\n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc\n",
        "\n",
        "batch_size = 10000\n",
        "\n",
        "# Split train dataset to mini batches\n",
        "X_train_mini_batches = torch.split(gloveX_train,batch_size)\n",
        "Y_train_mini_batches = torch.split(Y_train,batch_size)\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0\n",
        "  epoch_accuracy = 0\n",
        "  validation_loss=0\n",
        "  val_accuracy=0\n",
        "\n",
        "  for X_train_mini_batch,Y_train_mini_batch in zip(X_train_mini_batches,Y_train_mini_batches):\n",
        "\n",
        "    X_train_mini_batch = X_train_mini_batch.to(device)\n",
        "    Y_train_mini_batch = Y_train_mini_batch.to(device)\n",
        "\n",
        "    # Forward pass to get output\n",
        "    train_prediction = ff_model.forward(X_train_mini_batch.float())\n",
        "    train_prediction = torch.squeeze(train_prediction)\n",
        "\n",
        "    # Calculate Loss\n",
        "    train_loss = criterion(train_prediction,Y_train_mini_batch)\n",
        "\n",
        "    # Clearing up accumulated gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Getting gradients\n",
        "    train_loss.backward()\n",
        "\n",
        "    # Updating parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add each mini batch's loss\n",
        "    epoch_loss += train_loss.item()\n",
        "\n",
        "    # Add each mini batch's accuracy\n",
        "    epoch_accuracy += calculate_accuracy(Y_train_mini_batch,train_prediction)\n",
        "\n",
        "  # For some epochs print loss and accucary of train and validation set.\n",
        "  if epoch % 1 == 0:\n",
        "\n",
        "    gloveX_val = gloveX_val.to(device)\n",
        "    Y_val = Y_val.to(device)\n",
        "\n",
        "    # Forward pass to get output\n",
        "    val_prediction = ff_model.forward(gloveX_val.float())\n",
        "    val_prediction = torch.squeeze(val_prediction)\n",
        "\n",
        "    # Calculate Loss\n",
        "    val_loss = criterion(val_prediction,Y_val)\n",
        "\n",
        "    # Add each mini batch's loss\n",
        "    validation_loss += val_loss.item()\n",
        "\n",
        "    # Add each mini batch's accuracy\n",
        "    val_accuracy = calculate_accuracy(Y_val,val_prediction)\n",
        "\n",
        "    epoch_loss /= len(X_train_mini_batches)\n",
        "    epoch_accuracy /= len(X_train_mini_batches)\n",
        "    val_losses.append(validation_loss)\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "    val_accuracies.append(val_accuracy)"
      ],
      "metadata": {
        "id": "6JduhcyIi7t-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gloveX_test = gloveX_test.to(device)\n",
        "Y_test = Y_test.to(device)\n",
        "\n",
        "# Forward pass to get output\n",
        "test_prediction = ff_model.forward(gloveX_test.float())\n",
        "test_prediction = torch.squeeze(test_prediction)\n",
        "\n",
        "# Add each mini batch's accuracy\n",
        "test_accuracy = calculate_accuracy(Y_test,test_prediction)\n",
        "\n",
        "print(\"Test Accuracy:\",round(test_accuracy.item(),4)*100, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtUondcKkyg0",
        "outputId": "80a05fda-7582-418d-ea66-9ab1e1577157"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 68.64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Show the classification report\n",
        "test_prediction = test_prediction.to(device)\n",
        "test_prediction = test_prediction.ge(.5).view(-1).cpu()\n",
        "Y_test = Y_test.cpu()\n",
        "\n",
        "print(classification_report(Y_test,test_prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFGoleERk5xY",
        "outputId": "e453224a-c189-479e-d8e8-b81c8c64e853"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.68      0.72      0.70      1281\n",
            "         1.0       0.69      0.65      0.67      1219\n",
            "\n",
            "    accuracy                           0.69      2500\n",
            "   macro avg       0.69      0.69      0.69      2500\n",
            "weighted avg       0.69      0.69      0.69      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "making another pass with 150 epochs to see if accuracy changes"
      ],
      "metadata": {
        "id": "Tb4zM1x9t_mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensions of each layer and num of epochs.\n",
        "input_dim = gloveX_train.shape[1]\n",
        "hidden_dim_1 = 64\n",
        "hidden_dim_2 = 16\n",
        "output_dim = 1\n",
        "num_epochs = 150\n",
        "\n",
        "#define model\n",
        "ff2_model = Feed_forward_netModel(input_dim,hidden_dim_1,hidden_dim_2,output_dim)\n",
        "\n",
        "# Define loss function.\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define as optimizer Adam.\n",
        "optimizer = optim.Adam(ff2_model.parameters(),lr=1e-4,weight_decay=1e-4)\n",
        "\n",
        "# Transfer all the computation to GPU (cuda device).\n",
        "ff2_model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "PNCxbdKatcjd"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(y_true, y_pred):\n",
        "    y_pred = torch.round(y_pred)\n",
        "    correct = (y_true == y_pred).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "batch_size = 10000\n",
        "\n",
        "# Split train dataset to mini batches\n",
        "X_train_mini_batches = torch.split(gloveX_train, batch_size)\n",
        "Y_train_mini_batches = torch.split(Y_train, batch_size)\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Start training\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    validation_loss = 0\n",
        "    val_accuracy = 0\n",
        "\n",
        "    for X_train_mini_batch, Y_train_mini_batch in zip(X_train_mini_batches, Y_train_mini_batches):\n",
        "\n",
        "        X_train_mini_batch = X_train_mini_batch.to(device)\n",
        "        Y_train_mini_batch = Y_train_mini_batch.to(device)\n",
        "\n",
        "        # Forward pass to get output\n",
        "        train_prediction = ff2_model.forward(X_train_mini_batch.float())\n",
        "        train_prediction = torch.squeeze(train_prediction)\n",
        "\n",
        "        # Calculate Loss\n",
        "        train_loss = criterion(train_prediction, Y_train_mini_batch)\n",
        "\n",
        "        # Clearing up accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Getting gradients\n",
        "        train_loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add each mini batch's loss\n",
        "        epoch_loss += train_loss.item()\n",
        "\n",
        "        # Add each mini batch's accuracy\n",
        "        epoch_accuracy += calculate_accuracy(Y_train_mini_batch, train_prediction)\n",
        "\n",
        "    # For some epochs print loss and accuracy of train and validation set.\n",
        "    if epoch % 1 == 0:\n",
        "\n",
        "        gloveX_val = gloveX_val.to(device)\n",
        "        Y_val = Y_val.to(device)\n",
        "\n",
        "        # Forward pass to get output\n",
        "        val_prediction = ff2_model.forward(gloveX_val.float())\n",
        "        val_prediction = torch.squeeze(val_prediction)\n",
        "\n",
        "        # Calculate Loss\n",
        "        val_loss = criterion(val_prediction, Y_val)\n",
        "\n",
        "        # Add each mini batch's loss\n",
        "        validation_loss += val_loss.item()\n",
        "\n",
        "        # Add each mini batch's accuracy\n",
        "        val_accuracy = calculate_accuracy(Y_val, val_prediction)\n",
        "\n",
        "        epoch_loss /= len(X_train_mini_batches)\n",
        "        epoch_accuracy /= len(X_train_mini_batches)\n",
        "        val_losses.append(validation_loss)\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n"
      ],
      "metadata": {
        "id": "94qXMHeht6PW"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gloveX_test = gloveX_test.to(device)\n",
        "Y_test = Y_test.to(device)\n",
        "\n",
        "# Forward pass to get output\n",
        "test_prediction = ff2_model.forward(gloveX_test.float())\n",
        "test_prediction = torch.squeeze(test_prediction)\n",
        "\n",
        "# Add each mini batch's accuracy\n",
        "test_accuracy = calculate_accuracy(Y_test,test_prediction)\n",
        "\n",
        "print(\"Test Accuracy:\",round(test_accuracy.item(),4)*100, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osw0_m2vuGiw",
        "outputId": "ce36aecd-8a7c-4c6e-b132-97756a5d5101"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 70.32000000000001 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Close the pickle files.\n",
        "x_train_file.close()\n",
        "y_train_file.close()\n",
        "x_test_file.close()\n",
        "y_test_file.close()\n",
        "x_val_file.close()\n",
        "y_val_file.close()"
      ],
      "metadata": {
        "id": "jpffNg6iveba"
      },
      "execution_count": 72,
      "outputs": []
    }
  ]
}